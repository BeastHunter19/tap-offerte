services:
  logstash:
    image: docker.elastic.co/logstash/logstash:9.0.1
    hostname: logstash
    volumes:
      - ./logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    environment:
      XPACK_MONITORING_ENABLED: "false"
    ports:
      - "5044:5044"
    depends_on:
      topics:
        condition: service_completed_successfully
  broker:
    image: apache/kafka:3.9.1
    hostname: broker
    container_name: broker
    ports:
      - '9092:9092'
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT_HOST://broker:9092,PLAINTEXT://broker:19092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker:29093'
      KAFKA_LISTENERS: 'CONTROLLER://:29093,PLAINTEXT_HOST://:9092,PLAINTEXT://:19092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      CLUSTER_ID: '81ec01f9-d33e-4f75-a302-4791512b850a'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
  topics:
    image: apache/kafka:3.9.1
    hostname: topics
    container_name: topics
    command: > 
      bash -c "
      /opt/kafka/bin/kafka-topics.sh --create --topic pdf-metadata --bootstrap-server broker:9092
      "
    depends_on:
      - broker
  crawler:
    build: ./crawler
    volumes:
      - shared-storage:/data
      - ./test_flyers:/tmp
    depends_on:
      - logstash
    environment:
      - LOGSTASH_HOST=logstash
  spark:
    build: ./spark
    hostname: spark
    container_name: spark
    secrets:
      - gemini_api_key
    environment:
      - GOOGLE_API_KEY_FILE=/run/secrets/gemini_api_key
    volumes:
      - sparklibs:/tmp/.ivy2
      - ./spark/app:/opt/tap
      - shared-storage:/data
    depends_on:
      topics:
        condition: service_completed_successfully
    command: >
      /opt/spark/bin/spark-submit
      --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp"
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6,org.elasticsearch:elasticsearch-spark-30_2.12:9.0.2
      /opt/tap/test.py
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.2
    mem_limit: 2 GB
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200"]
      interval: 5s
      timeout: 10s
      retries: 20
  index:
    image: curlimages/curl:latest
    depends_on:
      elasticsearch:
        condition: service_healthy
    volumes:
      - ./elasticsearch/index.json:/index.json:ro
      - ./elasticsearch/init-index.sh:/init-index.sh:ro
    environment:
      - INDEX_NAME=offers
      - ES_HOST=http://elasticsearch:9200
    entrypoint: [ "sh", "/init-index.sh" ]

volumes:
  shared-storage:
  sparklibs:

secrets:
  gemini_api_key:
    file: ./secrets/gemini_api_key.txt